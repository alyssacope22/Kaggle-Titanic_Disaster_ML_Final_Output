{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T12:02:37.156545Z","iopub.execute_input":"2024-04-02T12:02:37.156941Z","iopub.status.idle":"2024-04-02T12:02:37.697647Z","shell.execute_reply.started":"2024-04-02T12:02:37.156910Z","shell.execute_reply":"2024-04-02T12:02:37.696205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:40.139386Z","iopub.execute_input":"2024-04-02T12:02:40.140657Z","iopub.status.idle":"2024-04-02T12:02:40.181930Z","shell.execute_reply.started":"2024-04-02T12:02:40.140609Z","shell.execute_reply":"2024-04-02T12:02:40.180569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the name of for training data set\ntest_data.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:44.835365Z","iopub.execute_input":"2024-04-02T12:02:44.836870Z","iopub.status.idle":"2024-04-02T12:02:44.844696Z","shell.execute_reply.started":"2024-04-02T12:02:44.836831Z","shell.execute_reply":"2024-04-02T12:02:44.842716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the column names for the testing data set\ntrain_data.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:46.909074Z","iopub.execute_input":"2024-04-02T12:02:46.909460Z","iopub.status.idle":"2024-04-02T12:02:46.915688Z","shell.execute_reply.started":"2024-04-02T12:02:46.909429Z","shell.execute_reply":"2024-04-02T12:02:46.914432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check info of the training data set\ntrain_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:48.488446Z","iopub.execute_input":"2024-04-02T12:02:48.488815Z","iopub.status.idle":"2024-04-02T12:02:48.520808Z","shell.execute_reply.started":"2024-04-02T12:02:48.488783Z","shell.execute_reply":"2024-04-02T12:02:48.519795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check info of the test data set\ntest_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:50.595041Z","iopub.execute_input":"2024-04-02T12:02:50.595393Z","iopub.status.idle":"2024-04-02T12:02:50.606939Z","shell.execute_reply.started":"2024-04-02T12:02:50.595368Z","shell.execute_reply":"2024-04-02T12:02:50.605621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training dataset contains 12 columns and 891 rows of data. There are 7 numeric columns (5 numeric and 2 float), and 5 categorical columns. The test dataset contains 11 rows (no Survived column) and 418 rows. There are 5 numeric columns (4 numeric and 2 float), and 5 categorical columns.","metadata":{}},{"cell_type":"markdown","source":"## Running Descriptive Statistics and Correlations ","metadata":{}},{"cell_type":"code","source":"#Descriptive Statistics on all variables for training data set\ntrain_data.describe().style.background_gradient(cmap='Oranges')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:52.812115Z","iopub.execute_input":"2024-04-02T12:02:52.812977Z","iopub.status.idle":"2024-04-02T12:02:52.933281Z","shell.execute_reply.started":"2024-04-02T12:02:52.812938Z","shell.execute_reply":"2024-04-02T12:02:52.932165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Descriptive Statistics on all variables for training data set\ntest_data.describe().style.background_gradient(cmap='Oranges')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:54.689144Z","iopub.execute_input":"2024-04-02T12:02:54.690256Z","iopub.status.idle":"2024-04-02T12:02:54.724374Z","shell.execute_reply.started":"2024-04-02T12:02:54.690217Z","shell.execute_reply":"2024-04-02T12:02:54.722830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking at both of the training and testing data sets, they both look fairly similar as far as their descriptive statistics go between the same columns. Nothing stands out as a big difference for using the training dataset to predict the test dataset. There should be no issues there.\n\nFor Pclass, with a mean of approximately 2.3 for both datasets, a majority of the Titanic was made up of the 2nd and 3rd class, and the smaller population was 1st class. A large population of the passengers on the Titanic were between the ages of 20 to approximately 39 (~50%). For those with siblings and spouses (SibSp), most people did not have either and were traveling alone. For parents and children (Parch), most people did not have either and were traveling alone. As for the Fare, the mean was 33.92 and the median was 14.45 with the highest ticket price of 512.33 and lowest ticket price of 0.00. The mean and median prices should fall in line with the pricing for 3rd and 2nd class tickets that make up the majority of the Titanic passengers.","metadata":{}},{"cell_type":"code","source":"#Looking at the training data categorical variables\ntrain_data.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:57.914080Z","iopub.execute_input":"2024-04-02T12:02:57.915226Z","iopub.status.idle":"2024-04-02T12:02:57.936489Z","shell.execute_reply.started":"2024-04-02T12:02:57.915178Z","shell.execute_reply":"2024-04-02T12:02:57.935410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at the testing data categorical variables\ntest_data.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:02:59.983763Z","iopub.execute_input":"2024-04-02T12:02:59.984152Z","iopub.status.idle":"2024-04-02T12:03:00.004761Z","shell.execute_reply.started":"2024-04-02T12:02:59.984123Z","shell.execute_reply":"2024-04-02T12:03:00.003565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking at the training dataset and testing dataset Sex column, males make up approximately 64.1% of the population of the Titanic. Then for the Embarked column, between both datasets is approximately 68.5% embarked from Southampton on the Titanic. In both datasets, about 81.6% have unique ticket numbers.","metadata":{}},{"cell_type":"code","source":"#Creating a function to count and calculate percentage of missing data in datasets\n#Calculating the output of the counts and percentage of missing data for training datasets\ndef missing_value(train_data:pd.DataFrame):\n    \"\"\"function to print the missing value \"\"\"\n    missing_train_data=train_data.isna().sum()\n    total_record=train_data.shape[0]\n    perc_missing=round((missing_train_data/total_record)*100,2)\n    missing_train_data=pd.DataFrame(data={'columns_name':missing_train_data.index,\n                                  'num_missing':missing_train_data.values,\n                                  'perc_missing':perc_missing.values})\n\n    return missing_train_data.sort_values(by='perc_missing',ascending=False)\nmissing_value(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:03:05.325794Z","iopub.execute_input":"2024-04-02T12:03:05.326161Z","iopub.status.idle":"2024-04-02T12:03:05.348266Z","shell.execute_reply.started":"2024-04-02T12:03:05.326134Z","shell.execute_reply":"2024-04-02T12:03:05.346390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Counting and calculating percentage of missing data in the testing data\nmissing_value(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:03:08.254895Z","iopub.execute_input":"2024-04-02T12:03:08.255259Z","iopub.status.idle":"2024-04-02T12:03:08.270354Z","shell.execute_reply.started":"2024-04-02T12:03:08.255232Z","shell.execute_reply":"2024-04-02T12:03:08.268813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Within the training dataset and testing dataset, the Cabin column has the highest percent of missing data with 77-78% missing. Next for both datasets, Age has the highest amount of missing data with about 20% missing. Then both datasets have only 1-2 missing Fare data points. All other columns have no missing data. Thus, in the analysis of the data, Cabin will probably not be a useful feature due to so much missing data, and Age it is possible to use it and possible either eliminate those rows of data or fill them in with data.","metadata":{}},{"cell_type":"markdown","source":"Let's produce some correlation matrices on the numeric variables and the categorical variables to see which of them are possibly affecting survival on the Titanic to examine further and concentrate on.","metadata":{}},{"cell_type":"code","source":"#Create a training data set where the categorical variables are converted into numeric variables\ntrain_data_num = train_data.copy()\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_data_num[['Sex']] = train_data_num[['Sex']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\ntrain_data_num[['Ticket']] = train_data_num[['Ticket']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\ntrain_data_num[['Cabin']] = train_data_num[['Cabin']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\ntrain_data_num[['Embarked']] = train_data_num[['Embarked']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\n\n#Drop the Name column because it isn't needed\ncols_to_drop = ['PassengerId', 'Name']\ntrain_data_num.drop(cols_to_drop, axis=1, inplace=True)\n\n# printing Dataframe\ntrain_data_num","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:03:14.300140Z","iopub.execute_input":"2024-04-02T12:03:14.301639Z","iopub.status.idle":"2024-04-02T12:03:14.845471Z","shell.execute_reply.started":"2024-04-02T12:03:14.301598Z","shell.execute_reply":"2024-04-02T12:03:14.844264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_num.describe().style.background_gradient(cmap='Oranges')","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:07.099359Z","iopub.execute_input":"2024-04-01T16:36:07.100037Z","iopub.status.idle":"2024-04-01T16:36:07.144264Z","shell.execute_reply.started":"2024-04-01T16:36:07.100007Z","shell.execute_reply":"2024-04-01T16:36:07.143262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation matrix for training numeric variables\nimport pandas as pd\nimport numpy as np\n\ncorr = train_data_num.corr()\n\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\ncorr[mask] = np.nan\n(corr\n .style\n .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n .highlight_null(color='#f1f1f1')  # Color NaNs grey\n .format(precision=3))","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:09.480892Z","iopub.execute_input":"2024-04-01T16:36:09.481274Z","iopub.status.idle":"2024-04-01T16:36:09.510160Z","shell.execute_reply.started":"2024-04-01T16:36:09.481241Z","shell.execute_reply":"2024-04-01T16:36:09.508967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at this correlation matrix, there are decently strong correlations between the Survived variable and their Pclass (class within the Titanic), their Sex (males vs. female), and their Fare (ticket price - likely covariance with Pclass). There is a slightly strong correlation between the Survived variable and their Ticket (likely covariance with Pclass and Fare) and Embarked (port of embarkment). All other variables have a weak or no correlation to the Survived variable (Age, SibSp, Parch, Cabin). \n\n***Note that Age and Cabin are missing 17% and 78% of their data in the training set when this correlation matrix was done.\n\nLet's run a Principal Component Analysis to see how much variance is explained by these variables.","metadata":{}},{"cell_type":"markdown","source":"## Running Principal Component Analysis ","metadata":{}},{"cell_type":"code","source":"#Prepping the data for PCA and separating out the features and the target (Survived)\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_data_num['Age'] = train_data_num['Age'].fillna((train_data_num['Age'].mean()))\ntrain_data_num['Embarked'] = train_data_num['Embarked'].fillna((train_data_num['Embarked'].mean()))\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']\n\ncols_to_drop = ['Cabin']\ntrain_data_num.drop(cols_to_drop, axis=1, inplace=True)\n\n# Separating out the features\nx = train_data_num.loc[:, features].values\n\n# Separating out the target\ny = train_data_num.loc[:,['Survived']].values\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:12.108899Z","iopub.execute_input":"2024-04-01T16:36:12.109741Z","iopub.status.idle":"2024-04-01T16:36:12.122326Z","shell.execute_reply.started":"2024-04-01T16:36:12.109705Z","shell.execute_reply":"2024-04-01T16:36:12.120752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gather the principal components for the analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=8)\n\nprincipalComponents = pca.fit_transform(x)\n\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', 'principal component 6', 'principal component 7', 'principal component 8'])","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:14.497866Z","iopub.execute_input":"2024-04-01T16:36:14.498280Z","iopub.status.idle":"2024-04-01T16:36:14.752614Z","shell.execute_reply.started":"2024-04-01T16:36:14.498250Z","shell.execute_reply":"2024-04-01T16:36:14.751349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the final dataframe for the PCA with Survived variable\nfinalDf = pd.concat([principalDf, train_data_num[['Survived']]], axis = 1)\n\nfinalDf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:16.990908Z","iopub.execute_input":"2024-04-01T16:36:16.991637Z","iopub.status.idle":"2024-04-01T16:36:17.012693Z","shell.execute_reply.started":"2024-04-01T16:36:16.991592Z","shell.execute_reply":"2024-04-01T16:36:17.011357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Output of the variance each principal component has on Titanic Survivors. PC 1 has the highest variance, PC 2 has the 2nd highest, PC 3 has the 3rd highest, so on and so forth for the variance values.","metadata":{}},{"cell_type":"code","source":"pca.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:19.157141Z","iopub.execute_input":"2024-04-01T16:36:19.157878Z","iopub.status.idle":"2024-04-01T16:36:19.166082Z","shell.execute_reply.started":"2024-04-01T16:36:19.157845Z","shell.execute_reply":"2024-04-01T16:36:19.164917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking at 8 of the variables (excluding Cabin due to 78% missing data), 5 principal components explain more than 80% of the variance explained in Survived variable of the training data set. Meaning that when modeling the data, it more than likely will not be necessary to include all of the variables because they are not providing much variance. This will be taken into consideration as data is further prepared and data is selected for modeling.\n\nWe want to ensure to not overfit any model and to not include unnecessary variables.","metadata":{}},{"cell_type":"code","source":"#Due to missing so much data and no correlation, excluding Cabin from both data sets\n#Drop the Name column because it isn't needed\ncols_to_drop = ['Cabin', 'Name', 'PassengerId']\ntrain_data.drop(cols_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:20.902671Z","iopub.execute_input":"2024-04-01T16:36:20.903050Z","iopub.status.idle":"2024-04-01T16:36:20.909760Z","shell.execute_reply.started":"2024-04-01T16:36:20.903024Z","shell.execute_reply":"2024-04-01T16:36:20.908458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the Different Variables of the Training Data Set ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize']=(6,6)\n\nax = sns.countplot(train_data,x='Survived', hue = 'Survived', dodge=False)\n\nfor c in ax.containers:\n    \n    # custom label calculates percent and add an empty string so 0 value bars don't have a number\n    labels = [f'{h/train_data.Survived.count()*100:0.1f}%' if (h := v.get_height()) > 0 else '' for v in c]\n    \n    ax.bar_label(c, labels=labels, label_type='edge')\n    \nplt.xlabel('Not Survived vs Survived')\nplt.suptitle('Counts and Rates of Titanic Survivors')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:23.198843Z","iopub.execute_input":"2024-04-01T16:36:23.199223Z","iopub.status.idle":"2024-04-01T16:36:23.642449Z","shell.execute_reply.started":"2024-04-01T16:36:23.199197Z","shell.execute_reply":"2024-04-01T16:36:23.641222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1,ax2) =plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nsns.barplot(x ='Sex', y ='Survived', data = train_data, palette ='plasma')\nax1.set_title(\"Rate of Titanic Survivors by their Sex\")\n\nplt.subplot(1,2,2)\nsns.barplot(x ='Pclass', y ='Survived', data = train_data, palette ='plasma')\nplt.xlabel('Titanic Passenger Class')\nax2.set_title(\"Rate of Titanic Survivors by their Class\")\n\n#plt.subplot(1,2,3)\n#sns.barplot(x ='Embarked', y ='Survived', data = train_data, palette ='plasma')\n#plt.xlabel('Titanic Passenger Class')\n#ax3.set_title(\"Rate of Titanic Survivors by Port of Embarkment\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:42:10.712993Z","iopub.execute_input":"2024-04-01T17:42:10.713423Z","iopub.status.idle":"2024-04-01T17:42:11.206815Z","shell.execute_reply.started":"2024-04-01T17:42:10.713395Z","shell.execute_reply":"2024-04-01T17:42:11.205509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the graphs and comparing it to the correlation matrix, women are much significantly more likely to survive than men on the Titanic. When it comes to class, the first class citizens had a higher likelihood of surviving than 2nd class and a much higher survivorship than 3rd class citizens; which makes sense since 3rd class citizens where located in the lower portions of the ships for their cabins.","metadata":{}},{"cell_type":"code","source":"fig, (ax1,ax2) =plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nsns.barplot(x ='Embarked', y ='Survived', data = train_data, palette ='plasma')\nplt.xlabel('Embarkment')\nax1.set_title(\"Rate of Titanic Survivors by Port of Embarkment\")\n\nplt.subplot(1,2,2)\nsns.boxplot(y='Age',x='Survived',data = train_data)\nax2.set_title(\"Titanic Survivors by their Age\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:53:37.745870Z","iopub.execute_input":"2024-04-01T17:53:37.746366Z","iopub.status.idle":"2024-04-01T17:53:38.204717Z","shell.execute_reply.started":"2024-04-01T17:53:37.746331Z","shell.execute_reply":"2024-04-01T17:53:38.203624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking at the Port of Embarkment, those who embarked from Cherbourg have the highest survivorship on the Titanic and those from Southampton have the lowest survivorship. This could have to do with being related to class or ticket or fare but that is speculation.\n\nThen looking at age, there is almost no discernible difference between age and survivorship on the Titanic. There appears to be that maybe people that are slightly older didn't survive as much as those who were a bit younger, but it is not much. There is too much overlap between the data. It is not significant enough which falls in line with the lack of correlation in the matrix.","metadata":{}},{"cell_type":"code","source":"fig, (ax1,ax2) =plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nsns.barplot(x ='SibSp', y ='Survived', data = train_data, palette ='plasma')\nplt.xlabel('Sibling/Spouse')\nax1.set_title(\"Rate of Titanic Survivors with Siblings and/or Spouses\")\n\nplt.subplot(1,2,2)\nsns.barplot(x='Parch',y='Survived',data = train_data, palette = 'plasma')\nplt.xlabel('Parent/Child')\nax2.set_title(\"Titanic Survivors with Parents and/or Chilren\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T18:02:16.032653Z","iopub.execute_input":"2024-04-01T18:02:16.033102Z","iopub.status.idle":"2024-04-01T18:02:17.256066Z","shell.execute_reply.started":"2024-04-01T18:02:16.033070Z","shell.execute_reply":"2024-04-01T18:02:17.255105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking at the Titanic survivors with siblings/spouses, there is a difference in survivors with no siblings/spouses and 1 sibling spouse. However, after that, any other data points, there is so much overlap that there is no discernible difference. It is hard to give a pattern or confirm any significance, which falls in line with the correlation matrix.\n\nLooking at the Titanic survivors with parent/children, there is a difference between 0 and 1-2 parent/children. Beyond that, all other data points have so much overlap there is no discernible difference. It is hard to give a pattern or confirm any significance. Again, this falls in line with the correlation matrix.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(6,6)  \nsns.boxplot(y='Fare',x='Survived',data = train_data)\nplt.suptitle('Titanic Counts of Suvivors by Ticket Fare')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T18:09:06.826386Z","iopub.execute_input":"2024-04-01T18:09:06.827047Z","iopub.status.idle":"2024-04-01T18:09:07.016480Z","shell.execute_reply.started":"2024-04-01T18:09:06.827009Z","shell.execute_reply":"2024-04-01T18:09:07.015383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While there is a lot of overlap happening with outlier data points, when it comes to looking at the min, max, median, 1st quartile, and 3rd quartile, those who paid a lower ticket fare had a lower likelihood of surviving. In those that didn't survive, about 50% of tickets averaging between 7.86 - 26.00 and a median of 10.80. In those who did survive, about 50% of the  tickets averaging between 12.50 - 57.00 and a median of 26.00. This graph does show differences between survivorship and ticket fare which is connected to the correlation matrix.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the Data for the XGBoost Model ","metadata":{}},{"cell_type":"code","source":"#Have 3 main data sets to work with for modeling. Train data as it currently is\n#Split data into features (X) and target variable (y)\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_data_num1 = train_data.copy()\ntest_data_num1 = test_data.copy()\n\ncols_to_drop = ['Name', 'Cabin', 'PassengerId']\ntest_data_num1.drop(cols_to_drop, axis=1, inplace=True)\n\ntrain_data_num1[['Sex']] = train_data_num1[['Sex']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\ntrain_data_num1[['Ticket']] = train_data_num1[['Ticket']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\n\ntest_data_num1[['Sex']] = test_data_num1[['Sex']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\ntest_data_num1[['Ticket']] = test_data_num1[['Ticket']].apply(lambda col:pd.Categorical(col).codes).replace(-1,np.nan)\n\nX_main = train_data_num1.drop('Survived', axis=1)\ny_main = train_data_num1['Survived']","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:26.126738Z","iopub.execute_input":"2024-04-01T16:36:26.127618Z","iopub.status.idle":"2024-04-01T16:36:26.147881Z","shell.execute_reply.started":"2024-04-01T16:36:26.127582Z","shell.execute_reply":"2024-04-01T16:36:26.146875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train data excluding Age, SibSp, Parch\n#Drop the Name column because it isn't needed\ncols_to_drop = ['Age', 'SibSp', 'Parch']\ntrain_sub1 = train_data_num1.drop(cols_to_drop, axis=1)\n\ntest_data_num2 = test_data_num1.copy()\ncols_to_drop = ['Age', 'SibSp', 'Parch']\ntest_data_num2 = test_data_num2.drop(cols_to_drop, axis=1)\n\n#Split data into features (X) and target variable (y)\nX_sub1 = train_sub1.drop('Survived', axis=1)\ny_sub1 = train_sub1['Survived']","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:36:28.856098Z","iopub.execute_input":"2024-04-01T16:36:28.856518Z","iopub.status.idle":"2024-04-01T16:36:28.866209Z","shell.execute_reply.started":"2024-04-01T16:36:28.856485Z","shell.execute_reply":"2024-04-01T16:36:28.865048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train data excluding Age\n#Drop the Name column because it isn't needed\ncols_to_drop = ['Age']\ntrain_sub2 = train_data_num1.drop(cols_to_drop, axis=1)\n\ntest_data_num3 = test_data_num1.copy()\ncols_to_drop = ['Age']\ntest_data_num3 = test_data_num3.drop(cols_to_drop, axis=1)\n\n#Split data into features (X) and target variable (y)\nX_sub2 = train_sub2.drop('Survived', axis=1)\ny_sub2 = train_sub2['Survived']","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:20.262809Z","iopub.execute_input":"2024-04-01T16:37:20.263599Z","iopub.status.idle":"2024-04-01T16:37:20.272887Z","shell.execute_reply.started":"2024-04-01T16:37:20.263544Z","shell.execute_reply":"2024-04-01T16:37:20.271513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take the training data set and split it from the X and y creation\n#Take this step with the train_main data set\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:25.801224Z","iopub.execute_input":"2024-04-01T16:37:25.801622Z","iopub.status.idle":"2024-04-01T16:37:25.810528Z","shell.execute_reply.started":"2024-04-01T16:37:25.801596Z","shell.execute_reply":"2024-04-01T16:37:25.809178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take the training data set and split it from the X and y creation\n#Take this step with the train_sub1 data set\nfrom sklearn.model_selection import train_test_split\nX_train_sub1, X_val_sub1, y_train_sub1, y_val_sub1 = train_test_split(X_sub1, y_sub1, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:27.580889Z","iopub.execute_input":"2024-04-01T16:37:27.581282Z","iopub.status.idle":"2024-04-01T16:37:27.591314Z","shell.execute_reply.started":"2024-04-01T16:37:27.581254Z","shell.execute_reply":"2024-04-01T16:37:27.590120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take the training data set and split it from the X and y creation\n#Take this step with the train_sub2 data set\nfrom sklearn.model_selection import train_test_split\nX_train_sub2, X_val_sub2, y_train_sub2, y_val_sub2 = train_test_split(X_sub2, y_sub2, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:29.106815Z","iopub.execute_input":"2024-04-01T16:37:29.107216Z","iopub.status.idle":"2024-04-01T16:37:29.116176Z","shell.execute_reply.started":"2024-04-01T16:37:29.107189Z","shell.execute_reply":"2024-04-01T16:37:29.114838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running the XGBoost Model with All Variables (sans Cabin feature) ","metadata":{}},{"cell_type":"code","source":"#Encode categorical variables - training main data\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Identify categorical columns\ncategorical_cols = ['Embarked']\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(drop='first', sparse=False)\n\n# Fit encoder on training data\nencoder.fit(X_train[categorical_cols])\n\n# Transform categorical columns\nencoded_cols_train = pd.DataFrame(encoder.transform(X_train[categorical_cols]))\nencoded_cols_val = pd.DataFrame(encoder.transform(X_val[categorical_cols]))\nencoded_cols_test = pd.DataFrame(encoder.transform(test_data_num1[categorical_cols]))\n\n# Reindexing encoded columns to match original indices\nencoded_cols_train.index = X_train.index\nencoded_cols_val.index = X_val.index\nencoded_cols_test.index = test_data_num1.index\n\n# Drop original categorical columns and concatenate encoded ones\nX_train_encoded = pd.concat([X_train.drop(categorical_cols, axis=1), encoded_cols_train], axis=1)\nX_val_encoded = pd.concat([X_val.drop(categorical_cols, axis=1), encoded_cols_val], axis=1)\ntest_data_encoded = pd.concat([test_data_num1.drop(categorical_cols, axis=1), encoded_cols_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:30.853502Z","iopub.execute_input":"2024-04-01T16:37:30.853923Z","iopub.status.idle":"2024-04-01T16:37:30.880225Z","shell.execute_reply.started":"2024-04-01T16:37:30.853894Z","shell.execute_reply":"2024-04-01T16:37:30.879020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Model Training (Using this model because it can handle missing data) - Using the largest model with main training data\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train_encoded, y_train)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = model.predict(X_val_encoded)\naccuracy = accuracy_score(y_val, y_pred)\n\nprint(\"Validation Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:58:40.355172Z","iopub.execute_input":"2024-04-01T16:58:40.355625Z","iopub.status.idle":"2024-04-01T16:58:40.731532Z","shell.execute_reply.started":"2024-04-01T16:58:40.355579Z","shell.execute_reply":"2024-04-01T16:58:40.730401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The XGBoost model accuracy for using all the Titanic variables besides the Cabin variable shows an accuracy of 82.1% which is a decent accuracy. I want to re-run the model where I exclude Age, Sibsp, and Parch to see if there is an improvement in the model accuracy per the correlation matrix and the PCA variance output.\n\nBelow are the predictions for the first XGBoost model and the possible submission output.","metadata":{}},{"cell_type":"code","source":"#XGBoost Model Predictions on Test Data from Main Training Data\ntest_predictions1 = model.predict(test_data_encoded)\n#print(test_predictions1)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:58:42.312470Z","iopub.execute_input":"2024-04-01T16:58:42.313507Z","iopub.status.idle":"2024-04-01T16:58:42.322963Z","shell.execute_reply.started":"2024-04-01T16:58:42.313467Z","shell.execute_reply":"2024-04-01T16:58:42.321713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create Possible Submission File\nsubmission1 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': test_predictions1})","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:37.757024Z","iopub.execute_input":"2024-04-01T16:37:37.757405Z","iopub.status.idle":"2024-04-01T16:37:37.762785Z","shell.execute_reply.started":"2024-04-01T16:37:37.757378Z","shell.execute_reply":"2024-04-01T16:37:37.761979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rerunning the Model with Eliminated Variables that Show No Correlation and No PCA Variance","metadata":{}},{"cell_type":"markdown","source":"Preparing the data for the second XGBoost model like we did for the first model using the second test data set that was preparted (training sub1).","metadata":{}},{"cell_type":"code","source":"#Encode categorical variables - training sub1 data\nfrom sklearn.preprocessing import OneHotEncoder\n\n#Identify categorical columns\ncategorical_cols1 = ['Embarked']\n\n#Initialize OneHotEncoder\nencoder = OneHotEncoder(drop='first', sparse=False)\n\n#Fit encoder on training sub1 data\nencoder.fit(X_train_sub1[categorical_cols1])\n\n#Transform categorical columns for the training sub1 data\nencoded_cols_trains1 = pd.DataFrame(encoder.transform(X_train_sub1[categorical_cols1]))\nencoded_cols_vals1 = pd.DataFrame(encoder.transform(X_val_sub1[categorical_cols1]))\nencoded_cols_tests1 = pd.DataFrame(encoder.transform(test_data_num2[categorical_cols1]))\n\n#Reindexing encoded columns to match original indices\nencoded_cols_trains1.index = X_train_sub1.index\nencoded_cols_vals1.index = X_val_sub1.index\nencoded_cols_tests1.index = test_data_num2.index\n\n#Drop original categorical columns and concatenate encoded ones\nX_train_encodeds1 = pd.concat([X_train_sub1.drop(categorical_cols, axis=1), encoded_cols_trains1], axis=1)\nX_val_encodeds1 = pd.concat([X_val_sub1.drop(categorical_cols, axis=1), encoded_cols_vals1], axis=1)\ntest_data_encodeds1 = pd.concat([test_data_num2.drop(categorical_cols1, axis=1), encoded_cols_tests1], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:42.347451Z","iopub.execute_input":"2024-04-01T16:37:42.348222Z","iopub.status.idle":"2024-04-01T16:37:42.368057Z","shell.execute_reply.started":"2024-04-01T16:37:42.348182Z","shell.execute_reply":"2024-04-01T16:37:42.366848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Model Training Part 2 - Using the model with sub1 training data\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train_encodeds1, y_train_sub1)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred2 = model.predict(X_val_encodeds1)\naccuracy2 = accuracy_score(y_val_sub1, y_pred2)\n\nprint(\"Validation Accuracy:\", accuracy2)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:58:51.350811Z","iopub.execute_input":"2024-04-01T16:58:51.351206Z","iopub.status.idle":"2024-04-01T16:58:51.432205Z","shell.execute_reply.started":"2024-04-01T16:58:51.351178Z","shell.execute_reply":"2024-04-01T16:58:51.431199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 2nd XGBoost model with Sex, Pclass, Fare, Ticket, and Embarked has a higher model accuracy of 83.8% than the first model which includes all variables except for Cabin. So, far, this is strongest model with the highest accuracy and is falling in line with what was shown in the correlation matrix and the PCA variance output. \n\nLet's re-run the model one last time, including SibSp and Parch but excluding Age. This is to exclude all variables that are missing a significant amount of data to see if there is any improvement or change in the model.\n\nBelow are the output and possible submission file of the second XGBoost model.","metadata":{}},{"cell_type":"code","source":"#XGBoost Model Predictions on Test Data from Main Training Data\ntest_predictions2 = model.predict(test_data_encodeds1)\n#print(test_predictions2)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:58:52.625105Z","iopub.execute_input":"2024-04-01T16:58:52.625532Z","iopub.status.idle":"2024-04-01T16:58:52.635966Z","shell.execute_reply.started":"2024-04-01T16:58:52.625503Z","shell.execute_reply":"2024-04-01T16:58:52.634692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create Possible Submission File\nsubmission2 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': test_predictions2})","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:37:50.402866Z","iopub.execute_input":"2024-04-01T16:37:50.403278Z","iopub.status.idle":"2024-04-01T16:37:50.409097Z","shell.execute_reply.started":"2024-04-01T16:37:50.403250Z","shell.execute_reply":"2024-04-01T16:37:50.407925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rerunning the Model with Only Age Removed Now (Checking for Improvement) ","metadata":{}},{"cell_type":"markdown","source":"Preparing the 3rd training data set for the XGBoost model the same as what was done for the main and sub1 training data sets (using trainin sub2).","metadata":{}},{"cell_type":"code","source":"#Encode categorical variables - training sub1 data\nfrom sklearn.preprocessing import OneHotEncoder\n\n#Identify categorical columns\ncategorical_cols2 = ['Embarked']\n\n#Initialize OneHotEncoder\nencoder = OneHotEncoder(drop='first', sparse=False)\n\n#Fit encoder on training sub1 data\nencoder.fit(X_train_sub2[categorical_cols2])\n\n#Transform categorical columns for the training sub1 data\nencoded_cols_trains2 = pd.DataFrame(encoder.transform(X_train_sub2[categorical_cols2]))\nencoded_cols_vals2 = pd.DataFrame(encoder.transform(X_val_sub2[categorical_cols2]))\nencoded_cols_tests2 = pd.DataFrame(encoder.transform(test_data_num3[categorical_cols2]))\n\n#Reindexing encoded columns to match original indices\nencoded_cols_trains2.index = X_train_sub2.index\nencoded_cols_vals2.index = X_val_sub2.index\nencoded_cols_tests2.index = test_data_num3.index\n\n#Drop original categorical columns and concatenate encoded ones\nX_train_encodeds2 = pd.concat([X_train_sub2.drop(categorical_cols2, axis=1), encoded_cols_trains2], axis=1)\nX_val_encodeds2 = pd.concat([X_val_sub2.drop(categorical_cols2, axis=1), encoded_cols_vals2], axis=1)\ntest_data_encodeds2 = pd.concat([test_data_num3.drop(categorical_cols2, axis=1), encoded_cols_tests2], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:40:33.751420Z","iopub.execute_input":"2024-04-01T16:40:33.751896Z","iopub.status.idle":"2024-04-01T16:40:33.777315Z","shell.execute_reply.started":"2024-04-01T16:40:33.751865Z","shell.execute_reply":"2024-04-01T16:40:33.776162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Model Training Part 3 - Using the model with sub2 training data\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train_encodeds2, y_train_sub2)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred3 = model.predict(X_val_encodeds2)\naccuracy3 = accuracy_score(y_val_sub2, y_pred3)\n\nprint(\"Validation Accuracy:\", accuracy3)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:58:56.264060Z","iopub.execute_input":"2024-04-01T16:58:56.265277Z","iopub.status.idle":"2024-04-01T16:58:56.348488Z","shell.execute_reply.started":"2024-04-01T16:58:56.265233Z","shell.execute_reply":"2024-04-01T16:58:56.347525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model where only Age and Cabin are removed (these are the variables missing a considerable amount of data), and the model accuracy is 83.8%. Which is the same accuracy as the XGBoost model with Age, Cabin, Sibsp, and Parch variables removed. \n\nMeaning that the model that will be used for submission will be the model using Pclass, Sex, Fare, Embarked, and Ticket variables (2nd model) because it is just as strong and a more simple model that explains the variance and is not overfitting the data. Having an accuracy of 83.8% is a decent level of accuracy given the variables. I did not fill in the missing data because I did not want to misrepresent any of the data and did not want to overfit any data in the modeling.","metadata":{}},{"cell_type":"code","source":"#XGBoost Model Predictions on Test Data from Main Training Data\ntest_predictions3 = model.predict(test_data_encodeds2)\n#print(test_predictions3)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:58:58.166609Z","iopub.execute_input":"2024-04-01T16:58:58.167013Z","iopub.status.idle":"2024-04-01T16:58:58.177680Z","shell.execute_reply.started":"2024-04-01T16:58:58.166985Z","shell.execute_reply":"2024-04-01T16:58:58.176363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create Possible Submission File\nsubmission3 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': test_predictions3})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Submission Selection for Competition ","metadata":{}},{"cell_type":"code","source":"submission2.head(15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Final Submission File for Competition\nsubmission2.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}